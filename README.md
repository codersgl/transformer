# Transformer

## Dataset

## self-attention

### Scale Dot-Product Attention

$$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})$$

## multi-head-self-attention

## multi-self

## Encoder

## Decoder
